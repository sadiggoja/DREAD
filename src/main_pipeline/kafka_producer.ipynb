{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e88c53c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from confluent_kafka import Producer\n",
    "import csv\n",
    "import json\n",
    "from datetime import datetime\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "dbdc9b24",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 53\u001b[0m\n\u001b[1;32m     50\u001b[0m                 time\u001b[38;5;241m.\u001b[39msleep(\u001b[38;5;241m120\u001b[39m)\n\u001b[1;32m     52\u001b[0m \u001b[38;5;66;03m# Trigger the function to send data\u001b[39;00m\n\u001b[0;32m---> 53\u001b[0m \u001b[43msend_data_to_kafka\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     55\u001b[0m \u001b[38;5;66;03m# Wait for any outstanding messages to be delivered and delivery reports to be received\u001b[39;00m\n\u001b[1;32m     56\u001b[0m producer\u001b[38;5;241m.\u001b[39mflush()\n",
      "Cell \u001b[0;32mIn[10], line 50\u001b[0m, in \u001b[0;36msend_data_to_kafka\u001b[0;34m()\u001b[0m\n\u001b[1;32m     48\u001b[0m counter\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m\n\u001b[1;32m     49\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m(counter\u001b[38;5;241m%\u001b[39m\u001b[38;5;241m400\u001b[39m\u001b[38;5;241m==\u001b[39m\u001b[38;5;241m0\u001b[39m):\n\u001b[0;32m---> 50\u001b[0m     \u001b[43mtime\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msleep\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m120\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Set up the Kafka producer configuration\n",
    "conf = {\n",
    "    'bootstrap.servers': 'localhost:9093',  # Adjust the Kafka broker address as needed\n",
    "}\n",
    "producer = Producer(conf)\n",
    "topic = 'spark_stream'\n",
    "    \n",
    "csv_file_path = '/home/jbara/Desktop/ost_project/final/powerconsumption.csv'\n",
    "\n",
    "\n",
    "# Function to read the CSV file and send data to Kafka\n",
    "def send_data_to_kafka():\n",
    "    with open(csv_file_path, 'r') as csvfile:\n",
    "        csvreader = csv.DictReader(csvfile)\n",
    "        counter=0\n",
    "        for row in list(csvreader)[:8000]:\n",
    "            \n",
    "            # Create a dictionary for the JSON payload\n",
    "            json_payload = {\n",
    "                #'measurement': 'measurement1',\n",
    "                #'tags': {'tag1': 'value1', 'tag2': 'value2'},\n",
    "                #'fields': {\n",
    "                    'Datetime': row['Datetime'],\n",
    "                    'Temperature': float(row['Temperature']),\n",
    "                    'Humidity': float(row['Humidity']),\n",
    "                    'WindSpeed': float(row['WindSpeed']),\n",
    "                    'GeneralDiffuseFlows': float(row['GeneralDiffuseFlows']),\n",
    "                    'DiffuseFlows': float(row['DiffuseFlows']),\n",
    "                    'PowerConsumption_Zone1': float(row['PowerConsumption_Zone1']),\n",
    "                    'PowerConsumption_Zone2': float(row['PowerConsumption_Zone2']),\n",
    "                    'PowerConsumption_Zone3': float(row['PowerConsumption_Zone3'])\n",
    "                }\n",
    "                \n",
    "\n",
    "            # Convert the dictionary to a JSON string\n",
    "            json_string = json.dumps(json_payload)\n",
    "\n",
    "            # Produce the message to Kafka\n",
    "            \n",
    "            \n",
    "            \n",
    "            producer.produce(topic, key=None, value=json_string)\n",
    "            counter+=1\n",
    "            if(counter%400==0):\n",
    "                time.sleep(120)\n",
    "            producer.flush()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ea329a28",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Trigger the function to send data\n",
    "send_data_to_kafka()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef4b766f-9b6b-4138-b849-73a5d6af6f88",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c16aedb8-95e0-4333-82a9-828ddac86b76",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
